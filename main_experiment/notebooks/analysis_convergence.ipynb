{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d80034dd",
   "metadata": {},
   "source": [
    "# Analysis of convergence regarding BDS and HS-DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc8bd4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b2980e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0d8d3ff6-7d93-497f-8ae7-3c5ca11b3fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau\n",
    "from scipy.stats import kendalltau, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "93e79bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"dog\", \"face\", \"tiny\", \"adult\"]\n",
    "classes = [4,4,5,5]\n",
    "scenario = [\"homo\", \"hetero\", \"hybrid\"]\n",
    "AI_accs = [\"-sigma\",\"mean\",\"+sigma\",\"max\"]\n",
    "r_values = [2,3,5,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3028818e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading dog with r=2: dog (or the experimental conditions do not exist.)\n",
      "Error reading face with r=2: face (or the experimental conditions do not exist.)\n",
      "Error reading face with r=10: face (or the experimental conditions do not exist.)\n",
      "Error reading tiny with r=3: tiny (or the experimental conditions do not exist.)\n",
      "Error reading tiny with r=5: tiny (or the experimental conditions do not exist.)\n",
      "Error reading tiny with r=10: tiny (or the experimental conditions do not exist.)\n",
      "Error reading adult with r=2: adult (or the experimental conditions do not exist.)\n",
      "Error reading adult with r=10: adult (or the experimental conditions do not exist.)\n"
     ]
    }
   ],
   "source": [
    "variable_count = {}\n",
    "for i,dataset in enumerate(datasets):\n",
    "    variable_count[dataset] = {}\n",
    "    for r in r_values:\n",
    "        try:\n",
    "            human_data = pl.read_csv(f\"../human_responses/{dataset}_r={r}.csv\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {dataset} with r={r}: {dataset} (or the experimental conditions do not exist.)\")\n",
    "            continue\n",
    "        variable_count[dataset][r] = {}\n",
    "        variable_count[dataset][r][\"n_classes\"] = classes[i]\n",
    "        variable_count[dataset][r][\"n_workers\"] = human_data[\"worker\"].n_unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5597fa80",
   "metadata": {},
   "source": [
    "## Ranking Metrics\n",
    ">In MCMC, the convergence of each latent variable can be diag-\n",
    "nosed without access to ground-truth data using the Gelman-Rubin\n",
    "statistic ($\\hat{R}$). Therefore, we analyzed the aggregation results of\n",
    "BDS and HS-DS_MCMC by calculating the number of latent variables\n",
    "that met the convergence criterion ($\\hat{R}$ < 1.1). For each experimental\n",
    "condition, we investigated the relationship between convergence\n",
    "and performance within these five runs. Specifically, we ranked the\n",
    "five outcomes for each condition based on two metrics: the number\n",
    "of converged variables and the final recall score. We then computed\n",
    "the correlation between these two rankings. This rank correlation\n",
    "analysis revealed a critical distinction between the two methods. For\n",
    "BDS, we found no meaningful correlation between the rankings,\n",
    "with average correlation coefficients being negligible (Kendall’s\n",
    "$\\tau$ = 0.11, Spearman’s $\\rho$ = 0.12). In contrast, HS-DS exhibited a mod-\n",
    "erate positive correlation (Kendall’s $\\tau$ = 0.42, Spearman’s $rho$ = 0.45),\n",
    "indicating that a higher degree of convergence is associated with\n",
    "better recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "79e35a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for dataset in datasets:\n",
    "    for r in r_values:\n",
    "        for sc in scenario:\n",
    "            for ai_acc in AI_accs:\n",
    "                try:\n",
    "                    data = pl.read_csv(f\"../results/{dataset}_{sc}_{r}_{ai_acc}.csv\")\n",
    "                except Exception as e:\n",
    "                    #print(f\"Error reading {dataset} with r={r}, sc={sc}, ai_acc={ai_acc}: {e}\")\n",
    "                    continue\n",
    "                uc_cols = [col for col in data.columns if col.startswith(\"uc_\")]\n",
    "                # Add a new column 'uc_sum' as the sum of uc_cols\n",
    "                data = data.with_columns([\n",
    "                    pl.sum_horizontal(uc_cols).alias(\"uc_sum\")\n",
    "                ])\n",
    "                data = data.filter(pl.col(\"uc_sum\") >= 0)\n",
    "\n",
    "                for num_ai in data[\"num_ai\"].unique():\n",
    "                    for iter in [0,1,2,3,4]:\n",
    "                        subset = data.filter(pl.col(\"num_ai\") == num_ai)\n",
    "                        subset = subset.filter(pl.col(\"iter\") == iter)\n",
    "                        for s in [1000,2000,3000]:\n",
    "                            row = {\n",
    "                                \"dataset\": dataset,\n",
    "                                \"r\": r,\n",
    "                                \"sc\": sc,\n",
    "                                \"ai_acc\": ai_acc,\n",
    "                                \"num_ai\": num_ai,\n",
    "                                \"s\": s,\n",
    "                                \"iter\": iter,\n",
    "                            }\n",
    "                            for method in [\"BDS\", \"HSDS_MCMC\"]:\n",
    "                                tmp_row = row.copy()\n",
    "                                tmp_row[\"method\"] = method \n",
    "                                tmp_row[\"uc_sum\"] = subset.filter(pl.col(\"method\") == f\"{method}(iter_sampling={s})\")[\"uc_sum\"].item()\n",
    "                                tmp_row[\"ucc_perc\"] = tmp_row[\"uc_sum\"] / (\n",
    "                                    (\n",
    "                                    (variable_count[dataset][r][\"n_workers\"] + num_ai) \\\n",
    "                                        * variable_count[dataset][r][\"n_classes\"] \\\n",
    "                                        * variable_count[dataset][r][\"n_classes\"]\n",
    "                                    ) + variable_count[dataset][r][\"n_classes\"]\n",
    "                                )\n",
    "                                tmp_row[\"recall\"] = subset.filter(pl.col(\"method\") == f\"{method}(iter_sampling={s})\")[\"recall\"].item()\n",
    "                                rows.append(tmp_row)\n",
    "rdf = pl.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a4c07b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf = rdf.with_columns((1-pl.col(\"ucc_perc\")).alias(\"cc_perc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9c6e1081",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf = rdf.with_columns([\n",
    "    (-pl.col(\"recall\")).rank(\"average\").over([\"dataset\", \"r\", \"sc\", \"ai_acc\",\"num_ai\" ,\"s\", \"method\"]).alias(\"rank_recall\"),\n",
    "    (-pl.col(\"cc_perc\")).rank(\"average\").over([\"dataset\", \"r\", \"sc\", \"ai_acc\", \"num_ai\", \"s\", \"method\"]).alias(\"rank_cc_perc\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b17ea164",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_cols = [\"dataset\", \"r\", \"sc\", \"ai_acc\", \"num_ai\", \"s\", \"method\"]\n",
    "results = []\n",
    "\n",
    "for group, df_group in rdf.group_by(group_cols, maintain_order=True):\n",
    "    ranks_recall = df_group[\"rank_recall\"].to_numpy()\n",
    "    ranks_cc_perc = df_group[\"rank_cc_perc\"].to_numpy()\n",
    "    if len(ranks_recall) > 1 and len(ranks_cc_perc) > 1:\n",
    "        tau, p_value = kendalltau(ranks_recall, ranks_cc_perc)\n",
    "        spear, spear_p = spearmanr(ranks_recall, ranks_cc_perc)\n",
    "        result = dict(zip(group_cols, group))\n",
    "        result[\"kendall_tau\"] = tau\n",
    "        result[\"kendall_p_value\"] = p_value\n",
    "        result[\"spearman_r\"] = spear\n",
    "        result[\"spearman_p_value\"] = spear_p\n",
    "        results.append(result)\n",
    "\n",
    "kendall_df = pl.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "167b51cf-b27a-4f34-81e6-db76188fd522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10708938807487796"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kendall_df.filter(\n",
    "    (pl.col(\"kendall_tau\").is_not_nan()) & \n",
    "    (pl.col(\"method\") == \"BDS\")\n",
    ")[\"kendall_tau\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "237e647c-0b75-429e-86a5-b53af0917047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12425438700566988"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kendall_df.filter(\n",
    "    (pl.col(\"spearman_r\").is_not_nan()) & \n",
    "    (pl.col(\"method\") == \"BDS\")\n",
    ")[\"spearman_r\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "75a37520-2e5c-4ca5-a6bc-11ed6e8836d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4247345070761007"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kendall_df.filter(\n",
    "    (pl.col(\"kendall_tau\").is_not_nan()) & \n",
    "    (pl.col(\"method\") == \"HSDS_MCMC\")\n",
    ")[\"kendall_tau\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d8c30cce-11c7-439f-9e8f-5d9e9fa486fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44931502298196047"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kendall_df.filter(\n",
    "    (pl.col(\"spearman_r\").is_not_nan()) & \n",
    "    (pl.col(\"method\") == \"HSDS_MCMC\")\n",
    ")[\"spearman_r\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f495118-4605-49e9-9967-34384c4fdfb1",
   "metadata": {},
   "source": [
    "## Average Improvements\n",
    "> Reinforcing this finding, under the same experimental settings, HS-DS had, on average, 34% fewer unconverged variables than BDS, while its recall was 0.43 higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eb2f87c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for dataset in datasets:\n",
    "    for r in r_values:\n",
    "        for sc in scenario:\n",
    "            for ai_acc in AI_accs:\n",
    "                try:\n",
    "                    data = pl.read_csv(f\"../results/{dataset}_{sc}_{r}_{ai_acc}.csv\")\n",
    "                except Exception as e:\n",
    "                    #print(f\"Error reading {dataset} with r={r}, sc={sc}, ai_acc={ai_acc}: {e}\")\n",
    "                    continue\n",
    "                uc_cols = [col for col in data.columns if col.startswith(\"uc_\")]\n",
    "                # Add a new column 'uc_sum' as the sum of uc_cols\n",
    "                data = data.with_columns([\n",
    "                    pl.sum_horizontal(uc_cols).alias(\"uc_sum\")\n",
    "                ])\n",
    "                data = data.group_by([\"method\", \"num_ai\"]).agg([\n",
    "                    pl.mean(\"uc_sum\").alias(\"avg_uc_sum\"),\n",
    "                    pl.mean(\"accuracy\").alias(\"avg_accuracy\"),\n",
    "                    pl.mean(\"recall\").alias(\"avg_recall\"),\n",
    "                ])\n",
    "                data = data.filter(pl.col(\"avg_uc_sum\") >= 0)\n",
    "                for num_ai in data[\"num_ai\"].unique():\n",
    "                    subset = data.filter(pl.col(\"num_ai\") == num_ai)\n",
    "                    for s in [1000,2000,3000]:\n",
    "                        row = {\n",
    "                            \"dataset\": dataset,\n",
    "                            \"r\": r,\n",
    "                            \"sc\": sc,\n",
    "                            \"ai_acc\": ai_acc,\n",
    "                            \"num_ai\": num_ai,\n",
    "                            \"s\": s,\n",
    "                        }\n",
    "                        row[\"ucc_bds\"] = subset.filter(pl.col(\"method\") == f\"BDS(iter_sampling={s})\")[\"avg_uc_sum\"].item()\n",
    "                        row[\"ucc_hsds\"] = subset.filter(pl.col(\"method\") == f\"HSDS_MCMC(iter_sampling={s})\")[\"avg_uc_sum\"].item()\n",
    "                        row[\"ucc_diff\"] = row[\"ucc_bds\"] - row[\"ucc_hsds\"]\n",
    "                        row[\"ucc_diff_perc\"] = row[\"ucc_diff\"] / (\n",
    "                            (\n",
    "                            (variable_count[dataset][r][\"n_workers\"] + num_ai) \\\n",
    "                                * variable_count[dataset][r][\"n_classes\"] \\\n",
    "                                * variable_count[dataset][r][\"n_classes\"]\n",
    "                            ) + variable_count[dataset][r][\"n_classes\"]\n",
    "                        )\n",
    "                        row[\"bds_accuracy\"] = subset.filter(pl.col(\"method\") == f\"BDS(iter_sampling={s})\")[\"avg_accuracy\"].item()\n",
    "                        row[\"bds_recall\"] = subset.filter(pl.col(\"method\") == f\"BDS(iter_sampling={s})\")[\"avg_recall\"].item()\n",
    "                        row[\"hsds_accuracy\"] = subset.filter(pl.col(\"method\") == f\"HSDS_MCMC(iter_sampling={s})\")[\"avg_accuracy\"].item()\n",
    "                        row[\"hsds_recall\"] = subset.filter(pl.col(\"method\") == f\"HSDS_MCMC(iter_sampling={s})\")[\"avg_recall\"].item()\n",
    "                        row[\"accuracy_diff\"] = row[\"hsds_accuracy\"] - row[\"bds_accuracy\"]\n",
    "                        row[\"recall_diff\"] = row[\"hsds_recall\"] - row[\"bds_recall\"]\n",
    "                        rows.append(row)\n",
    "results_df = pl.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1c98fc38-ffe9-4f40-81f6-e29ededb51cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1,)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>ucc_diff_perc</th></tr><tr><td>f64</td></tr></thead><tbody><tr><td>0.344028</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1,)\n",
       "Series: 'ucc_diff_perc' [f64]\n",
       "[\n",
       "\t0.344028\n",
       "]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.mean()[\"ucc_diff_perc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1983b745-e5b5-429f-bb84-c5b97765d5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1,)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>recall_diff</th></tr><tr><td>f64</td></tr></thead><tbody><tr><td>0.433607</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1,)\n",
       "Series: 'recall_diff' [f64]\n",
       "[\n",
       "\t0.433607\n",
       "]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.mean()[\"recall_diff\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
